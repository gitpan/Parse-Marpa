=head1 NAME

Parse::Marpa::Doc::Algorithm - The Marpa Algorithm

=head1 DESCRIPTION

Marpa is derived from
the parser
L<described by John Aycock and R.  Nigel Horspool|Parse::Marpa::Doc::Bibliography/"Aycock and Horspool 2002">.
Aycock and Horspool combined LR(0)
precomputation
with the general parsing algorithm
L<described by Jay
Earley in 1970|Parse::Marpa::Doc::Bibliography/"Earley 1970">.

Marpa combines many parsing techniques,
and a full description would be a textbook.
I have to content myself
with briefly describing the more significant ideas new with Marpa.
In this document
I assume that the reader is familiar with parsing in general,
and that he understands terms like Earley set and Earley item.
It will be helpful to have read the other Marpa documents,
up to and including L<Parse::Marpa::Doc::Internals>.

All claims of originality are limited by my ignorance.
The parsing literature is large, and I may have been preceded by someone whose
work I'm not aware of.
Also, as readers familiar with the academic literature will know,
a claim that combining A with B is original with Marpa,
is B<not> to be read as a claim to be that either A or B
is original with Marpa.

=head2 A New Recognition Engine

Marpa takes the recognition engine as
described by Horspool and Aycock
and turns it inside out and upside down.
In the Horspool and Aycock version of Earley's,
the main loop iterated over each item of an Earley set,
first scanning for tokens,
then completing items in the set.
Marpa turns this into two separate loops over the
Earley items,
the first of which completes the current earley set,
and the second of which scans for tokens.

=head3 Predictive Lexing and Earley Parsing

The advantage of this double inversion is that the lexical analysis
can be put between the two loops
-- after completion,
but before scanning.
This makes predictive lexing possible.
Being able to predict which lexables are legal at a given
point in the parse can save a lot of processing,
especially if the lexing is complex.
Any lexemes
which are not predicted by 
items in the current
Earley set do not need to be scanned for.
Now that Hayspool and Aycock have sped up Earley's algorithm,
the time spent lexing is a significant factor in overall speed.
Predictive lexing can reduce lexing time to a fraction of the original.

=head2 Earlemes

Marpa allows ambiguous lexing,
including recognition of lexemes of different lengths starting at the
same lexical position,
and recognition of overlapping lexemes.
To facilitate this,
Marpa introduces the B<earleme> (named after Jay Earley).
Previous Earley 
implementations required the input to be broken
up into tokens, usually by lexical analysis of the input using
DFA's.
(DFA's -- deterministic finite automata -- are the equivalent of
regular expressions when that term is used in the strictest sense).
Requiring that the first level of analysis
be performed by a DFA hobbles a general parser like Earley's.

Marpa allows the scanning phase of
Earley's algorithm to add items not just to the
next Earley set, but to any later one.
Alternative scans of the input can be put into the Earley sets, and
the power of Earley's algorithm harnessed to deal with the
indeterminism.

Marpa does this by allowing
each scanned item to have a length in earlemes.
Call that length I<L>.
If the current Earley set is I<I>, a newly scanned
Earley item is added to Earley set I<I+L>.
From this point of view,
the B<earleme> is a unit of distance
measured in Earley sets.
In the same way that distances in miles can be seen as locations
when they are associated with milestones,
the individual Earley sets can also be seen as earleme milestones.
The first Earley set is earleme zero.
Subsequent Earley sets are called earleme 1, earleme 2, etc.,
based on their distance in earlemes from earleme 0.

An implementation can sync
earlemes up with any measure that's convenient.
For example, the
distance in earlemes may be the length of a string, measured
either in bytes or Unicode graphemes.
An implementation may also mimic traditional lexing by defining
the earleme to be the same as the distance in a token stream,
measured in tokens.

=head2 Chomsky-Horspool-Aycock Form

Another significant change
to the Aycock and Horspool algorithm
in Marpa
is its internal rewriting
of the grammar.
Aycock and Horspool rewrote their grammars internally into what they
called NNF (Nihilist Normal Form).
Earley's original
algorithm had serious issues with nullable symbols and productions,
and the NNF rewrite fixes almost all of them.
(A nullable symbol or production is
one which derives the empty sentence.) Importantly,
NNF also allows complete and easy mapping of the semantics of the
original grammar to its NNF rewrite, so that NNF and the whole
rewrite process can be made invisible to the user.

A problem with NNF is that the rewritten grammar is
exponentially larger than the original in the theoretical worst
case.
I think
cases could arise in practice
where the NNF size explosion is a real problem.
One such case might be
Perl 6 rules in which whitespace is significant but optional.

Marpa's solution is Chomsky-Horspool-Aycock Form (CHAF).
This is
Horspool and Aycock's NNF, but with the further restriction that
no more than two nullable symbols may appear in any production.
(The idea that any context-free grammar can
be rewritten into productions of at most a small fixed size appears to
originate, like so much else, with Noam Chomsky.)
The shortened CHAF production maps back
to the original grammar, so that like NNF, the CHAF rewrite can be
made invisible to the user.
With CHAF, the theoretical worst
behavior is linear, and in those difficult cases likely to arise
in practice the multiplier is much smaller.

=head2 Iterating Aycock-Horspool Parses

Aycock and Horspool give an algorithm for constructing a rightmost derivation
from their version of the Earley sets.
They suggest that in the case of multiple parses,
their Earley sets could be iterated through,
and they point out where the decision points occur
in their algorithm.
But they give no algorithm to do that iteration.

Marpa's method of evaluation is to first create a B<parse bocage>
from the Earley sets.
Marpa generates the parse trees from the parse bocage.
Parse trees are kept as an array, in pre-order form.
Parse trees are evaluated by scanning the
array in reverse, so that they are seen in post-order
and a conventional evaluation stack can be used.

=head3 Parse Bocages

A ambiguous parse is a set of parse trees, and in the parsing literature
there is
an efficient and compact means of storing a set of closely related
parse trees.  It is called, aptly, a parse forest.
Nodes in
a parse forest are divided into and-nodes and or-nodes.
And-nodes are individual pieces of parse trees.
In conventional parse forests,
each and-node represents a production, as in a parse tree,
with the lhs as the parent node and the rhs symbols as child nodes.
Or-nodes represent choices between and-nodes,
with each child of an or-node being a choice.
A parse tree is generated from a parse forest
by traversing the forest and selecting one child node
at every or-node.

Marpa needed to adapt parse forests for two reasons.
First, Marpa parses may contain cycles, and for that reason, strictly
speaking, it is not a set of parse trees which needs to be represented
in the parse bocage,
but a set of parse graphs.

Second, the productions of the grammar are not repesented intact in the
Earley items.  Instead each production is broken up, symbol by symbol.
Aycock and Horspool used a system of links to allow a parse structure to
constructed from the Earley items.
Each Earley item has available one link to point to a child symbol,
and one to point to an adjacent symbol in its production.
Marpa's recognizer takes it system of links between Earley items
from Aycock and Horspool.

In effect, Aycock and Horspool's system of links transforms the grammar
into a variant of CNF (Chomsky Normal Form), where every rhs has at most
two symbols.
The CNF grammar can be represented as a binary tree.

Marpa could have restored the original structure of the grammar when
it created the parse bocage from the Earley items.
But the binary representation
in the Earley sets is more compact than a conventional
parse forest.
More importantly,
binary trees can be traversed, modified and evaluated more simply and efficiently
than ordinary trees.
It made sense to leave the grammar in a form close to that
in which it appears in the Earley items.

For this reason,
the and-nodes in parse bocages do not directly represent productions
of the grammar. 
Instead every and-node has at most two children.

The bocage forests of Normandy and Brittany are
networks of dense hedgerows cultivated over centuries
as obstacles to livestock and armies.
These hedges are impressive obstacles even
to modern infantry, and the Norman bocage played a major role in World War II.
The term "parse bocage" seemed appropriate for
the strategically crafted tangle which
Marpa uses to store parses.

=head3 Iterating the Parse Trees

To find the next the parse tree in an ambiguous parse,
Marpa scans the last parse tree it generated,
comparing it to the parse bocage.
The parse tree, which is kept in pre-order in an array,
is treated as a stack, and scanned from the end, looking
for a node to iterate.

Because the pre-order array is scanned from the end,
the nodes of the parse tree are examined in post-order.
As each node is encountered, it is compared with the parse bocage.
Every node in the parse tree corresponds both to an or-node,
and to a current choice of and-node from that or-node.
If all the choices of and-node for a parse tree node
have already been tried,
the parse tree node is "popped" from the "parse tree stack".

When a parse tree node is reached
which has another choice of and-node available,
that parse tree node is selected as the interation node.
The next choice of and-node is made,
and a modified iteration node is pushed back onto the parse tree.
The child nodes of the iteration node are generated and pushed on
the parse tree "stack".
Finally, those nodes at the "top" of the original "stack" which were not
descendants of the iteration node,
and which therefore are unchanged,
are pushed back onto the parse tree "stack".

=head2 Precedence Parsing and Earley Parsing

In order to iterate through the parses,
Marpa stores, with most of the Earley items,
information about why it was
added.
Marpa uses lists of "links" and tokens for this,
expanding on ideas from Aycock and Horspool.
The order of these lists
controls the order of the parses,
and realized that the order of those lists could be changed to anything useful.
The order of the link and token lists might,
in fact, be based on priorities assigned by the user to the rules and tokens of the grammar.
These user-assigned priorities could then force the most desirable parse,
to be first in parsing order.

In effect,
Marpa's rule and terminal priorities turn ambiguous parsing
into a convenient form of precedence parsing.
I don't believe Earley parsing and precedence parsing have been combined before.

=head2 Quasi-Deterministic Finite Automata (QDFA's)

Aycock and Horspool's precomputation uses what they call
a split LR(0) E<epsilon>-DFA.
Not exactly the most transparent name,
but a brilliant concept:
if you leave in some of the non-determinism
when you transform an NFA to an LR(0) DFA,
you can use it to guide the Earley recognizer.

While the concept gave me an essential insight that I could
never have come up on my own,
I found the split LR(0) E<epsilon>-DFA awkward to program.
A point came when I wanted to add some more non-determinism,
and then I realized
that not only would an ordinary NFA allow my change,
but that it would make the code shorter and better.

What Marpa does, therefore, is to transform the original NFA
into a second NFA.
The second NFA keeps the form of an NFA, but has most of the
non-determinism taken out of it.
It's an NFA with a lot less "N".
It is almost an LR(0) DFA, so I call it a quasi-deterministic finite
automaton, or QDFA.

=head1 SUPPORT

See the L<support section|Parse::Marpa/SUPPORT> in the main module.

=head1 AUTHOR

Jeffrey Kegler

=head1 LICENSE AND COPYRIGHT

Copyright 2007 - 2008 Jeffrey Kegler

This program is free software; you can redistribute
it and/or modify it under the same terms as Perl 5.10.0.

=cut

# Local Variables:
#   mode: cperl
#   cperl-indent-level: 4
#   fill-column: 100
# End:
# vim: expandtab shiftwidth=4:
